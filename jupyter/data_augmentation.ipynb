{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmentation\n",
    "import os \n",
    "import numpy as np\n",
    "import torch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "DATA_PATH = os.path.abspath(\"../dataset/weifeng/all_graphs_2.pkl\")\n",
    "data = np.load(DATA_PATH, allow_pickle=True)\n",
    "\n",
    "\n",
    "# data_x \n",
    "x = [item['adj'] for item in data]\n",
    "x = np.array(x)\n",
    "x_shape = x.shape\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "y = [item['y'] for item in data]\n",
    "y = np.array(y)\n",
    "y = (y[:, 2] + y[:, 3] + y[:, 4] >= 1).astype(int)\n",
    "\n",
    "# split data\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "train_x_smote, train_y_smote = sm.fit_resample(train_x, train_y)\n",
    "\n",
    "# save data\n",
    "data = []\n",
    "for i in range(train_x.shape[0]):\n",
    "    data.append({'adj': train_x[i].reshape(x_shape[1], x_shape[2]), 'y': train_y[i]})\n",
    "with open(\"../dataset/weifeng/all_graphs_2_external_train.pkl\", 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "data = []\n",
    "for i in range(train_x_smote.shape[0]):\n",
    "    data.append({'adj': train_x_smote[i].reshape(x_shape[1], x_shape[2]), 'y': train_y_smote[i]})\n",
    "with open(\"../dataset/weifeng/all_graphs_2_external_train_smote.pkl\", 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "data = []\n",
    "for i in range(test_x.shape[0]):\n",
    "    data.append({'adj': test_x[i].reshape(x_shape[1], x_shape[2]), 'y': test_y[i]})\n",
    "with open(\"../dataset/weifeng/all_graphs_2_external_test.pkl\", 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/uvadm/zhenyu/miniconda3/envs/brain/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "# global mean pooling and global max pooling\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_sparse import SparseTensor\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "# from utils.utils import get_spectral_embedding\n",
    "from dgl.data import DGLDataset\n",
    "\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# set the path to the root directory of the project\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "DATA_PATH = os.path.abspath(\"../dataset/weifeng/all_graphs_2.pkl\")\n",
    "# DATA_PATH = os.path.abspath(\"../dataset/processed/data_dict_site16_binary.pkl\")\n",
    "\n",
    "\n",
    "def pre_transform(data: Dict[str, Any]) -> Data:\n",
    "    \"\"\"Transform the data into torch Data type\"\"\"\n",
    "    if \"all_graphs\" in  DATA_PATH :\n",
    "        x = torch.tensor(data['adj'])\n",
    "        adj = torch.tensor(data[\"adj\"], dtype=torch.float32)\n",
    "        adj = (adj - adj.min()) / (adj.max() - adj.min())\n",
    "        edge_index_ = (adj >= -1).nonzero().t().contiguous()\n",
    "        edge_index_ = edge_index_[:, edge_index_[0] != edge_index_[1]]\n",
    "        edge_weight = adj[edge_index_[0], edge_index_[1]]\n",
    "        try:\n",
    "            label = torch.tensor(data['y']).unsqueeze(0)\n",
    "        except:\n",
    "            label = torch.tensor(data['y'])\n",
    "        return Data(\n",
    "            x=x,\n",
    "            x_SC=None,\n",
    "            edge_index=edge_index_,\n",
    "            edge_weight=edge_weight,\n",
    "            edge_index_SC=None,\n",
    "            edge_weight_SC=None,\n",
    "            y=label\n",
    "        )\n",
    "    else:\n",
    "        x = torch.tensor(data[\"FC\"], dtype=torch.float32)\n",
    "        x_SC = torch.tensor(data[\"SC\"], dtype=torch.float32)\n",
    "        x_SC = (x_SC.max() - x_SC) / (x_SC.max() - x_SC.min())\n",
    "\n",
    "        edge_index_FC = (x >= 0.8).nonzero().t().contiguous()\n",
    "        edge_index_FC = edge_index_FC[:, edge_index_FC[0] != edge_index_FC[1]]\n",
    "        row, col = edge_index_FC\n",
    "        edge_weight_FC = x[row, col]\n",
    "\n",
    "        edge_index_SC =  (x_SC > x_SC.mean()).nonzero().t().contiguous()\n",
    "        edge_index_SC = edge_index_SC[:, edge_index_SC[0] != edge_index_SC[1]]\n",
    "        row, col = edge_index_SC\n",
    "        edge_weight_SC = torch.tensor(x_SC[row, col], dtype=torch.float32)\n",
    "\n",
    "        feature = torch.tensor(data['feature'], dtype=torch.float32).unsqueeze(0) if 'feature' in data.keys() else None\n",
    "        label_tensor = torch.tensor(data['label'], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "        return Data(\n",
    "            x=x,\n",
    "            x_SC=x_SC,\n",
    "            edge_index=edge_index_FC,\n",
    "            edge_weight=edge_weight_FC,\n",
    "            edge_index_SC=edge_index_SC,\n",
    "            edge_weight_SC=edge_weight_SC,\n",
    "            y=label_tensor,\n",
    "            feature=feature\n",
    "        )\n",
    "\n",
    "\n",
    "class Brain(InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        task,\n",
    "        x_attributes=None,\n",
    "        processed_path=\"../data/processed\",\n",
    "        rawdata_path=DATA_PATH,\n",
    "        suffix=None,\n",
    "        args=None,\n",
    "    ):\n",
    "        \n",
    "        if suffix is None:\n",
    "            suffix = \"\"\n",
    "        self.processed_path = os.path.join(processed_path, f\"{task}_data{suffix}.pt\")\n",
    "\n",
    "        self.task = task\n",
    "        self.x_attributes = x_attributes\n",
    "        self.rawdata_path = rawdata_path\n",
    "        if \"train\" in suffix or \"test\" in suffix:\n",
    "            self.rawdata_path = rawdata_path.replace(\".pkl\", suffix + \".pkl\")\n",
    "        self.suffix = suffix\n",
    "        self.pre_transform = pre_transform\n",
    "\n",
    "        super().__init__(pre_transform=self.pre_transform)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_path)\n",
    "\n",
    "        \"\"\"modify\"\"\"\n",
    "        # task = args.task_idx\n",
    "        # task = [1]\n",
    "        # self.data.y = (self.data.y)[:, task]\n",
    "        \"\"\"modify end\"\"\"\n",
    "\n",
    "\n",
    "    def processed_file_names(self):\n",
    "        return os.path.basename(self.processed_path)\n",
    "\n",
    "    def process(self) -> None:\n",
    "        with open(self.rawdata_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        data_list = []\n",
    "        for i in tqdm(range(len(data))):\n",
    "            if self.pre_transform is not None:\n",
    "                data_list.append(self.pre_transform(data[i]))\n",
    "        \n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "        print(\"Saving...\")\n",
    "        torch.save((self.data, self.slices), self.processed_path)\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return os.path.dirname(self.processed_path)\n",
    "\n",
    "    def process_data(self, data):\n",
    "        data_list = []\n",
    "        for i in range(len(data)):\n",
    "            try:\n",
    "                data_list.append(self.pre_transform(data[i]))\n",
    "            except:\n",
    "                pdb.set_trace()\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        print(\"Saving...\")\n",
    "        torch.save((data, slices), self.processed_path)\n",
    "        \n",
    "\n",
    "train_dataset_smote = Brain(task='classification', x_attributes=['adj'], suffix=\"_external_train_smote\")\n",
    "test_dataset = Brain(task='classification', x_attributes=['adj'], suffix=\"_external_test\")\n",
    "train_dataset = Brain(task='classification', x_attributes=['adj'], suffix=\"_external_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, global_mean_pool, global_max_pool, GCNConv, SAGEConv, GATConv, GatedGraphConv, SGConv, ResGatedGraphConv \n",
    "\n",
    "import pdb\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "\n",
    "class GIN_pyg(nn.Module):\n",
    "    def __init__(self, net_params, args):\n",
    "        super(GIN_pyg, self).__init__()\n",
    "        in_channels = net_params[\"in_channels\"]\n",
    "        hidden_channels = net_params[\"hidden_channels\"]\n",
    "        out_channels = net_params[\"out_channels\"]\n",
    "        num_layers = net_params[\"num_layers\"]\n",
    "        dropout = net_params[\"dropout\"]\n",
    "\n",
    "        self.readout_type = net_params[\"readout\"]\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.conv1 = GCNConv(self.in_channels, self.hidden_channels)\n",
    "        self.conv2 = GCNConv(self.hidden_channels, self.hidden_channels)\n",
    "        self.fc = nn.Linear(self.hidden_channels, 1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_weight.unsqueeze(-1), data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight)) \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        predict = self.fc(x)\n",
    "\n",
    "        return predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uqerh3vw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_AUC</td><td>█▄▅▅▅▆▅▄▃▂▂▂▁</td></tr><tr><td>train_AUC</td><td>▃▄▁▄▆▅▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▇▆▄▃▂▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_AUC</td><td>0.49538</td></tr><tr><td>train_AUC</td><td>0.62274</td></tr><tr><td>train_loss</td><td>2.30055</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train</strong> at: <a href='https://wandb.ai/vjd5zr/brain/runs/uqerh3vw' target=\"_blank\">https://wandb.ai/vjd5zr/brain/runs/uqerh3vw</a><br/> View project at: <a href='https://wandb.ai/vjd5zr/brain' target=\"_blank\">https://wandb.ai/vjd5zr/brain</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241222_115001-uqerh3vw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uqerh3vw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sfs/gpfs/tardis/project/uvadm/zhenyu/project/BrainProject/brain/jupyter/wandb/run-20241222_115502-254tk1co</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vjd5zr/brain/runs/254tk1co' target=\"_blank\">Train</a></strong> to <a href='https://wandb.ai/vjd5zr/brain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vjd5zr/brain' target=\"_blank\">https://wandb.ai/vjd5zr/brain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vjd5zr/brain/runs/254tk1co' target=\"_blank\">https://wandb.ai/vjd5zr/brain/runs/254tk1co</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/uvadm/zhenyu/miniconda3/envs/brain/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.09226593375206, AUC: 0.5353618421052632\n",
      "test_AUC: 0.5343589743589744\n",
      "Epoch: 1, Loss: 2.534818321466446, AUC: 0.5563322368421053\n",
      "test_AUC: 0.4902564102564103\n",
      "Epoch: 2, Loss: 2.3919230699539185, AUC: 0.6195175438596492\n",
      "test_AUC: 0.46461538461538465\n",
      "Epoch: 3, Loss: 2.390120804309845, AUC: 0.594640899122807\n",
      "test_AUC: 0.45743589743589746\n",
      "Epoch: 4, Loss: 2.3118708729743958, AUC: 0.6787280701754387\n",
      "test_AUC: 0.4502564102564103\n",
      "Epoch: 5, Loss: 2.2106895446777344, AUC: 0.7206688596491228\n",
      "test_AUC: 0.4461538461538462\n",
      "Epoch: 6, Loss: 2.2817825078964233, AUC: 0.7361567982456141\n",
      "test_AUC: 0.4574358974358974\n",
      "Epoch: 7, Loss: 2.180802345275879, AUC: 0.7804961622807017\n",
      "test_AUC: 0.4461538461538461\n",
      "Epoch: 8, Loss: 2.1407677084207535, AUC: 0.7540433114035088\n",
      "test_AUC: 0.4471794871794872\n",
      "Epoch: 9, Loss: 1.969567984342575, AUC: 0.828673245614035\n",
      "test_AUC: 0.45743589743589746\n",
      "Epoch: 10, Loss: 1.882045492529869, AUC: 0.8886376096491228\n",
      "test_AUC: 0.4533333333333333\n",
      "Epoch: 11, Loss: 1.7047531306743622, AUC: 0.9004248903508771\n",
      "test_AUC: 0.44512820512820517\n",
      "Epoch: 12, Loss: 1.5811203122138977, AUC: 0.9377055921052632\n",
      "test_AUC: 0.438974358974359\n",
      "Epoch: 13, Loss: 1.3345110565423965, AUC: 0.9513432017543859\n",
      "test_AUC: 0.4574358974358974\n",
      "Epoch: 14, Loss: 1.1892040222883224, AUC: 0.9663514254385965\n",
      "test_AUC: 0.4225641025641026\n",
      "Epoch: 15, Loss: 1.1173977181315422, AUC: 0.9771792763157896\n",
      "test_AUC: 0.44307692307692315\n",
      "Epoch: 16, Loss: 1.3092715814709663, AUC: 0.9473684210526315\n",
      "test_AUC: 0.3958974358974359\n",
      "Epoch: 17, Loss: 0.9939588755369186, AUC: 0.9841694078947368\n",
      "test_AUC: 0.39897435897435896\n",
      "Epoch: 18, Loss: 0.8076456412672997, AUC: 0.993626644736842\n",
      "test_AUC: 0.4379487179487179\n",
      "Epoch: 19, Loss: 0.6329791322350502, AUC: 0.995202850877193\n",
      "test_AUC: 0.4482051282051282\n",
      "Epoch: 20, Loss: 0.4027782939374447, AUC: 0.999109100877193\n",
      "test_AUC: 0.4328205128205128\n",
      "Epoch: 21, Loss: 0.26572736632078886, AUC: 1.0\n",
      "test_AUC: 0.4348717948717949\n",
      "Epoch: 22, Loss: 0.2829108200967312, AUC: 0.9995888157894738\n",
      "test_AUC: 0.4348717948717949\n",
      "Epoch: 23, Loss: 0.24994610622525215, AUC: 0.9991776315789473\n",
      "test_AUC: 0.41846153846153855\n",
      "Epoch: 24, Loss: 0.18149410840123892, AUC: 0.9996573464912281\n",
      "test_AUC: 0.4574358974358974\n",
      "Epoch: 25, Loss: 0.17084690928459167, AUC: 0.9996573464912281\n",
      "test_AUC: 0.4635897435897436\n",
      "Epoch: 26, Loss: 0.13104771682992578, AUC: 1.0\n",
      "test_AUC: 0.37025641025641026\n",
      "Epoch: 27, Loss: 0.13633683137595654, AUC: 1.0\n",
      "test_AUC: 0.4256410256410257\n",
      "Epoch: 28, Loss: 0.06064659729599953, AUC: 1.0\n",
      "test_AUC: 0.46974358974358976\n",
      "Epoch: 29, Loss: 0.02652732888236642, AUC: 1.0\n",
      "test_AUC: 0.4461538461538461\n",
      "Epoch: 30, Loss: 0.027898604865185916, AUC: 1.0\n",
      "test_AUC: 0.45333333333333337\n",
      "Epoch: 31, Loss: 0.02505502407439053, AUC: 1.0\n",
      "test_AUC: 0.4512820512820513\n",
      "Epoch: 32, Loss: 0.017479649046435952, AUC: 1.0\n",
      "test_AUC: 0.4461538461538461\n",
      "Epoch: 33, Loss: 0.011849494883790612, AUC: 1.0\n",
      "test_AUC: 0.4523076923076923\n",
      "Epoch: 34, Loss: 0.013026268221437931, AUC: 1.0\n",
      "test_AUC: 0.45948717948717954\n",
      "Epoch: 35, Loss: 0.010057904059067369, AUC: 1.0\n",
      "test_AUC: 0.4492307692307692\n",
      "Epoch: 36, Loss: 0.008626082562841475, AUC: 1.0\n",
      "test_AUC: 0.44717948717948725\n",
      "Epoch: 37, Loss: 0.006736731680575758, AUC: 1.0\n",
      "test_AUC: 0.44923076923076927\n",
      "Epoch: 38, Loss: 0.006396715878508985, AUC: 1.0\n",
      "test_AUC: 0.4502564102564103\n",
      "Epoch: 39, Loss: 0.005779451283160597, AUC: 1.0\n",
      "test_AUC: 0.45128205128205134\n",
      "Epoch: 40, Loss: 0.004584201727993786, AUC: 1.0\n",
      "test_AUC: 0.4512820512820513\n",
      "Epoch: 41, Loss: 0.004227883095154539, AUC: 1.0\n",
      "test_AUC: 0.45025641025641033\n",
      "Epoch: 42, Loss: 0.0033819113741628826, AUC: 1.0\n",
      "test_AUC: 0.4512820512820513\n",
      "Epoch: 43, Loss: 0.002935984550276771, AUC: 1.0\n",
      "test_AUC: 0.45230769230769236\n",
      "Epoch: 44, Loss: 0.0025713108479976654, AUC: 1.0\n",
      "test_AUC: 0.45128205128205134\n",
      "Epoch: 45, Loss: 0.002192421059589833, AUC: 1.0\n",
      "test_AUC: 0.4512820512820513\n",
      "Epoch: 46, Loss: 0.001609645623830147, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 47, Loss: 0.001395023675286211, AUC: 1.0\n",
      "test_AUC: 0.4564102564102564\n",
      "Epoch: 48, Loss: 0.001329899489064701, AUC: 1.0\n",
      "test_AUC: 0.4564102564102564\n",
      "Epoch: 49, Loss: 0.0010750822257250547, AUC: 1.0\n",
      "test_AUC: 0.45743589743589746\n",
      "Epoch: 50, Loss: 0.000942664424655959, AUC: 1.0\n",
      "test_AUC: 0.4574358974358975\n",
      "Epoch: 51, Loss: 0.0007998535620572511, AUC: 1.0\n",
      "test_AUC: 0.45641025641025645\n",
      "Epoch: 52, Loss: 0.0007083276504999958, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 53, Loss: 0.0006101474318711553, AUC: 1.0\n",
      "test_AUC: 0.45641025641025645\n",
      "Epoch: 54, Loss: 0.0005642302821797784, AUC: 1.0\n",
      "test_AUC: 0.45641025641025645\n",
      "Epoch: 55, Loss: 0.0005059240720584057, AUC: 1.0\n",
      "test_AUC: 0.45641025641025645\n",
      "Epoch: 56, Loss: 0.00046097721497062594, AUC: 1.0\n",
      "test_AUC: 0.4543589743589744\n",
      "Epoch: 57, Loss: 0.00043309265674906783, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 58, Loss: 0.00040970605914480984, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 59, Loss: 0.0003741577565961052, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 60, Loss: 0.0003997055100626312, AUC: 1.0\n",
      "test_AUC: 0.4543589743589744\n",
      "Epoch: 61, Loss: 0.00032438496782560833, AUC: 1.0\n",
      "test_AUC: 0.4543589743589744\n",
      "Epoch: 62, Loss: 0.0002918052523455117, AUC: 1.0\n",
      "test_AUC: 0.4543589743589744\n",
      "Epoch: 63, Loss: 0.0003318092676636297, AUC: 1.0\n",
      "test_AUC: 0.45641025641025645\n",
      "Epoch: 64, Loss: 0.0002660700938577065, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 65, Loss: 0.00024636867965455167, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 66, Loss: 0.0002531568952690577, AUC: 1.0\n",
      "test_AUC: 0.45333333333333337\n",
      "Epoch: 67, Loss: 0.00024301442863361444, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 68, Loss: 0.00024152306650648825, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 69, Loss: 0.00024242775907623582, AUC: 1.0\n",
      "test_AUC: 0.45641025641025645\n",
      "Epoch: 70, Loss: 0.00021455944079207256, AUC: 1.0\n",
      "test_AUC: 0.45641025641025645\n",
      "Epoch: 71, Loss: 0.00022061266463424545, AUC: 1.0\n",
      "test_AUC: 0.45641025641025645\n",
      "Epoch: 72, Loss: 0.00018564720448921435, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 73, Loss: 0.0001857828156062169, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 74, Loss: 0.00017481628128734883, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 75, Loss: 0.00015798298409208655, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 76, Loss: 0.00015208478362183087, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 77, Loss: 0.00016199810852413066, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 78, Loss: 0.00018626602650329005, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 79, Loss: 0.00015475000145670492, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 80, Loss: 0.0001326411465925048, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 81, Loss: 0.00012089385745639447, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 82, Loss: 0.00013295132521307096, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 83, Loss: 0.00011873781204485567, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 84, Loss: 0.00012393998622428626, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 85, Loss: 0.00011597474622249138, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 86, Loss: 0.00010379133982496569, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 87, Loss: 0.00010541694882704178, AUC: 1.0\n",
      "test_AUC: 0.45435897435897443\n",
      "Epoch: 88, Loss: 0.00010223327444691677, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 89, Loss: 0.00011151335820613895, AUC: 1.0\n",
      "test_AUC: 0.4543589743589743\n",
      "Epoch: 90, Loss: 0.00010903038764809025, AUC: 1.0\n",
      "test_AUC: 0.4543589743589743\n",
      "Epoch: 91, Loss: 9.015812793222722e-05, AUC: 1.0\n",
      "test_AUC: 0.4543589743589743\n",
      "Epoch: 92, Loss: 8.87431915543857e-05, AUC: 1.0\n",
      "test_AUC: 0.4543589743589743\n",
      "Epoch: 93, Loss: 9.62780950430897e-05, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 94, Loss: 9.222175231116125e-05, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 95, Loss: 7.877725238358835e-05, AUC: 1.0\n",
      "test_AUC: 0.4564102564102564\n",
      "Epoch: 96, Loss: 8.850694484863197e-05, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 97, Loss: 8.625930240668822e-05, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 98, Loss: 8.361057234651526e-05, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n",
      "Epoch: 99, Loss: 7.712430397077696e-05, AUC: 1.0\n",
      "test_AUC: 0.4553846153846154\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"brain\", name=\"Train\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "net_params = {\n",
    "    \"in_channels\": 379,\n",
    "    \"hidden_channels\": 64,\n",
    "    \"out_channels\": 1,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"readout\": \"mean\"\n",
    "}\n",
    "model = GIN_pyg(net_params, None).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# split data\n",
    "# train_ratio = 0.7\n",
    "# rand_idx = torch.randperm(len(dataset))\n",
    "# train_idx = rand_idx[:int(train_ratio * len(dataset))]\n",
    "# test_idx = rand_idx[int(train_ratio * len(dataset)):]\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in data_loader:\n",
    "        data = batch.to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            y_true.append(data.y.cpu().numpy())\n",
    "            y_pred.append(output.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    wandb.log({\"test_AUC\": auc})\n",
    "    print(f\"test_AUC: {auc}\")\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, test_loader):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        loss_all = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for batch in train_loader:\n",
    "            data = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output.squeeze(), data.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_all += loss.item()\n",
    "            y_true.append(data.y.cpu().numpy())\n",
    "            y_pred.append(output.detach().cpu().numpy())\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        wandb.log({\"train_loss\": loss_all, \"train_AUC\": auc})\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss_all}, AUC: {auc}\")\n",
    "        test(model, test_loader)\n",
    "        \n",
    "        \n",
    "    \n",
    "train(model, optimizer, criterion, train_loader, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. use SMOTE to augment train dataset will only worsen the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, global_mean_pool, global_max_pool, GCNConv, SAGEConv, GATConv, GatedGraphConv, SGConv, ResGatedGraphConv \n",
    "\n",
    "import pdb\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "\n",
    "class GIN_pyg_DANN(nn.Module):\n",
    "    def __init__(self, net_params, args):\n",
    "        super().__init__()\n",
    "        in_channels = net_params[\"in_channels\"]\n",
    "        hidden_channels = net_params[\"hidden_channels\"]\n",
    "        out_channels = net_params[\"out_channels\"]\n",
    "        num_layers = net_params[\"num_layers\"]\n",
    "        dropout = net_params[\"dropout\"]\n",
    "\n",
    "        self.readout_type = net_params[\"readout\"]\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.conv1 = GCNConv(self.in_channels, self.hidden_channels)\n",
    "        self.conv2 = GCNConv(self.hidden_channels, self.hidden_channels)\n",
    "        self.fc_class = nn.Linear(self.hidden_channels, 1)\n",
    "        self.fc_domain = nn.Linear(self.hidden_channels, 1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_weight.unsqueeze(-1), data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight)) \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        predict = self.fc_class(x)\n",
    "        domain = self.fc_domain(x)\n",
    "\n",
    "        return predict, domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s349jjfj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_AUC</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_AUC</td><td>0.54016</td></tr><tr><td>train_loss</td><td>12.23437</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DANN</strong> at: <a href='https://wandb.ai/vjd5zr/brain/runs/s349jjfj' target=\"_blank\">https://wandb.ai/vjd5zr/brain/runs/s349jjfj</a><br/> View project at: <a href='https://wandb.ai/vjd5zr/brain' target=\"_blank\">https://wandb.ai/vjd5zr/brain</a><br/>Synced 4 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241222_112026-s349jjfj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s349jjfj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sfs/gpfs/tardis/project/uvadm/zhenyu/project/BrainProject/brain/jupyter/wandb/run-20241222_112105-umkn0qth</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vjd5zr/brain/runs/umkn0qth' target=\"_blank\">DANN</a></strong> to <a href='https://wandb.ai/vjd5zr/brain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vjd5zr/brain' target=\"_blank\">https://wandb.ai/vjd5zr/brain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vjd5zr/brain/runs/umkn0qth' target=\"_blank\">https://wandb.ai/vjd5zr/brain/runs/umkn0qth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/uvadm/zhenyu/miniconda3/envs/brain/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 12.298264741897583, AUC: 0.5041803728070176\n",
      "test_AUC: 0.5897435897435898\n",
      "Epoch: 1, Loss: 11.973011493682861, AUC: 0.5061677631578947\n",
      "test_AUC: 0.5794871794871794\n",
      "Epoch: 2, Loss: 11.563602447509766, AUC: 0.5213130482456141\n",
      "test_AUC: 0.5764102564102564\n",
      "Epoch: 3, Loss: 11.146029949188232, AUC: 0.5134320175438597\n",
      "test_AUC: 0.5692307692307693\n",
      "Epoch: 4, Loss: 10.94868290424347, AUC: 0.5535910087719298\n",
      "test_AUC: 0.5671794871794872\n",
      "Epoch: 5, Loss: 10.711503863334656, AUC: 0.5563322368421053\n",
      "test_AUC: 0.5671794871794872\n",
      "Epoch: 6, Loss: 10.8728609085083, AUC: 0.5703125\n",
      "test_AUC: 0.5671794871794872\n",
      "Epoch: 7, Loss: 10.722944140434265, AUC: 0.5722313596491228\n",
      "test_AUC: 0.5641025641025641\n",
      "Epoch: 8, Loss: 10.820069432258606, AUC: 0.5782620614035089\n",
      "test_AUC: 0.5620512820512821\n",
      "Epoch: 9, Loss: 10.917051553726196, AUC: 0.5656524122807017\n",
      "test_AUC: 0.5610256410256411\n",
      "Epoch: 10, Loss: 10.53225064277649, AUC: 0.5790844298245613\n",
      "test_AUC: 0.557948717948718\n",
      "Epoch: 11, Loss: 10.606828093528748, AUC: 0.5808662280701754\n",
      "test_AUC: 0.561025641025641\n",
      "Epoch: 12, Loss: 10.929486274719238, AUC: 0.5744243421052632\n",
      "test_AUC: 0.561025641025641\n",
      "Epoch: 13, Loss: 10.839943170547485, AUC: 0.5786732456140351\n",
      "test_AUC: 0.562051282051282\n",
      "Epoch: 14, Loss: 10.5119469165802, AUC: 0.5810032894736842\n",
      "test_AUC: 0.5517948717948717\n",
      "Epoch: 15, Loss: 10.806590557098389, AUC: 0.5783991228070174\n",
      "test_AUC: 0.5497435897435898\n",
      "Epoch: 16, Loss: 10.751524090766907, AUC: 0.5736705043859649\n",
      "test_AUC: 0.5405128205128206\n",
      "Epoch: 17, Loss: 10.941154718399048, AUC: 0.5804550438596491\n",
      "test_AUC: 0.5312820512820513\n",
      "Epoch: 18, Loss: 10.687776446342468, AUC: 0.5816200657894737\n",
      "test_AUC: 0.5271794871794871\n",
      "Epoch: 19, Loss: 10.656038284301758, AUC: 0.5812774122807017\n",
      "test_AUC: 0.5251282051282051\n",
      "Epoch: 20, Loss: 10.67921245098114, AUC: 0.5801809210526315\n",
      "test_AUC: 0.5179487179487179\n",
      "Epoch: 21, Loss: 10.803963541984558, AUC: 0.5666118421052632\n",
      "test_AUC: 0.5138461538461538\n",
      "Epoch: 22, Loss: 11.115367770195007, AUC: 0.5833333333333333\n",
      "test_AUC: 0.5087179487179487\n",
      "Epoch: 23, Loss: 10.540253043174744, AUC: 0.5776452850877192\n",
      "test_AUC: 0.5097435897435898\n",
      "Epoch: 24, Loss: 10.610690593719482, AUC: 0.5844983552631579\n",
      "test_AUC: 0.5097435897435898\n",
      "Epoch: 25, Loss: 10.628384351730347, AUC: 0.5764802631578947\n",
      "test_AUC: 0.5076923076923077\n",
      "Epoch: 26, Loss: 10.677086472511292, AUC: 0.5800438596491228\n",
      "test_AUC: 0.5056410256410258\n",
      "Epoch: 27, Loss: 10.487436771392822, AUC: 0.5643503289473685\n",
      "test_AUC: 0.5056410256410258\n",
      "Epoch: 28, Loss: 10.663149952888489, AUC: 0.578125\n",
      "test_AUC: 0.5076923076923077\n",
      "Epoch: 29, Loss: 10.918121099472046, AUC: 0.5705180921052632\n",
      "test_AUC: 0.5230769230769231\n",
      "Epoch: 30, Loss: 10.77547800540924, AUC: 0.5653097587719298\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"brain\", name=\"DANN\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "net_params = {\n",
    "    \"in_channels\": 379,\n",
    "    \"hidden_channels\": 64,\n",
    "    \"out_channels\": 1,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"readout\": \"mean\"\n",
    "}\n",
    "model = GIN_pyg_DANN(net_params, None).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion_class = nn.BCEWithLogitsLoss()\n",
    "criterion_domain = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in data_loader:\n",
    "        data = batch.to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(data)\n",
    "            y_true.append(data.y.cpu().numpy())\n",
    "            y_pred.append(output.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    wandb.log({\"test_AUC\": auc})\n",
    "    print(f\"test_AUC: {auc}\")\n",
    "\n",
    "def train(model, optimizer, criterion_class, criterion_domain, train_loader, test_loader):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        loss_all = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for batch in train_loader:\n",
    "            data = batch.to(device)\n",
    "            # split src and tgt\n",
    "            src_idx = torch.randperm(len(data), device=device)[:int(3 * len(data) // 4)]\n",
    "            tgt_idx = torch.tensor(list(set(torch.arange(len(batch))) - set(src_idx))).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            class_output, domain_output = model(data)\n",
    "            \n",
    "\n",
    "            ## src\n",
    "            class_output_src = class_output[src_idx]\n",
    "            domain_output_src = domain_output[src_idx]\n",
    "            domain_label_src = torch.zeros(len(src_idx))\n",
    "            domain_label_src = domain_label_src.to(device)\n",
    "            class_loss = criterion_class(class_output_src.squeeze(-1), batch.y.float()[:len(src_idx)])\n",
    "            domain_loss_src = criterion_domain(domain_output_src.squeeze(-1), domain_label_src)\n",
    "\n",
    "            ## tgt\n",
    "            class_output_tgt = class_output[tgt_idx]\n",
    "            domain_output_tgt = domain_output[tgt_idx]\n",
    "            domain_label_tgt = torch.ones(len(tgt_idx))\n",
    "            domain_label_tgt = domain_label_tgt.to(device)\n",
    "\n",
    "            domain_loss_tgt = criterion_domain(domain_output_tgt.squeeze(-1), domain_label_tgt)\n",
    "\n",
    "            loss = class_loss + domain_loss_src + domain_loss_tgt\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_all += loss.item()\n",
    "            y_true.append(data.y.cpu().numpy())\n",
    "            y_pred.append(class_output.detach().cpu().numpy())\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        wandb.log({\"train_loss\": loss_all, \"train_AUC\": auc}, commit=False)\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss_all}, AUC: {auc}\")\n",
    "        test(model, test_loader)\n",
    "        \n",
    "        \n",
    "\n",
    "train(model, optimizer, criterion_class, criterion_domain, train_loader, test_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
